{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea1b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamilton_ice.pipeline import get_func_args, object_io_nodes, build_pipeline\n",
    "from hamilton_ice.io.artifact import artifact\n",
    "from hamilton_ice.io.pandas import pandas_csv_source, pandas_msgpack\n",
    "from hamilton_ice.io.dummy import dummy\n",
    "from hamilton_ice.util.graphviz import dag_plot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from hamilton_ice.util.graphviz import dag_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548d6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = None\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    observed_size = 0\n",
    "    reservoir_size = 50\n",
    "    random_seed = 42\n",
    "\n",
    "@dataclass\n",
    "class TrainingValidationMinibatch:\n",
    "    train : pd.DataFrame\n",
    "    validation : pd.DataFrame\n",
    "        \n",
    "@dataclass\n",
    "class AnnotatedLoss:\n",
    "    loss : float\n",
    "    validation_loss : float\n",
    "\n",
    "class Titanic:\n",
    "    @artifact\n",
    "    def params():\n",
    "        return {}\n",
    "    \n",
    "    @artifact\n",
    "    def config():\n",
    "        return Config()\n",
    "    \n",
    "    @pandas_csv_source\n",
    "    def minibatch(config):\n",
    "        return \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"    \n",
    "        \n",
    "\n",
    "        \n",
    "    @dummy\n",
    "    def normalized_data(minibatch, config):\n",
    "        dat = minibatch\n",
    "        normalized = pd.DataFrame({\n",
    "            \"Survived\" : dat[\"Survived\"].astype(float),\n",
    "            \"NormPclass\" : dat[\"Pclass\"]/3,\n",
    "            \"MaleSex\" : (dat[\"Sex\"] == \"male\").astype(float),\n",
    "            \"FemaleSex\" : (dat[\"Sex\"] == \"female\").astype(float),\n",
    "            \"NormAge\" : dat[\"Age\"] / 100,\n",
    "            \"NormSibSp\" :dat[\"SibSp\"] /10,\n",
    "            \"NormParch\" : dat[\"Parch\"]/10,\n",
    "            \"NormFare\" : dat[\"Fare\"]/1000,\n",
    "            \"Embarked_C\" : (dat[\"Embarked\"] == \"C\").astype(float),\n",
    "            \"Embarked_S\" : (dat[\"Embarked\"] == \"S\").astype(float),\n",
    "            \"Embarked_Q\" : (dat[\"Embarked\"] == \"Q\").astype(float),\n",
    "        })\n",
    "        mean_value = normalized[\"NormAge\"].mean(skipna=True)\n",
    "        normalized[\"NormAge\"].fillna(mean_value, inplace=True)\n",
    "        yield normalized\n",
    "    \n",
    "    @dummy\n",
    "    def annotated_data(normalized_data, config):\n",
    "        np.random.seed(config.random_seed)\n",
    "        msk = np.random.rand(len(normalized_data)) < 0.8\n",
    "        normalized_data[\"train\"] = msk\n",
    "        normalized_data[\"eval\"] = ~msk\n",
    "        yield normalized_data\n",
    "    \n",
    "    @dummy    \n",
    "    def evaluate(annotated_data):  \n",
    "        yield annotated_data[annotated_data[\"eval\"]]\n",
    "    \n",
    "    \n",
    "    @dummy                                 \n",
    "    def annotated_train(annotated_data, config):\n",
    "        train = annotated_data[annotated_data[\"train\"]].drop([\"train\", \"eval\"], axis = 1).copy()\n",
    "        \n",
    "        global validation\n",
    "        \n",
    "        k = random.randint(0, config.reservoir_size)\n",
    "        if validation is None:\n",
    "            validation = train.copy()\n",
    "        elif k == 1:\n",
    "            # swap an element with reservoir\n",
    "            val_idx = random.randint(0, len(validation))\n",
    "            validation.iloc[val_idx] = train.sample().copy()\n",
    "\n",
    "        yield TrainingValidationMinibatch(train=train, validation=validation)\n",
    "        \n",
    "    \n",
    "    @artifact\n",
    "    def optimizer(model):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.001)\n",
    "        return optimizer\n",
    "    \n",
    "    @artifact\n",
    "    def criterion(config):\n",
    "        return torch.nn.BCELoss() \n",
    "    \n",
    "    \n",
    "    \n",
    "    @dummy\n",
    "    def train(annotated_train, model, optimizer, criterion):\n",
    "        train = annotated_train.train.drop([\"Survived\"], 1).to_numpy()\n",
    "        train_features = torch.tensor(train)\n",
    "        train_outputs = model(train_features.float())\n",
    "        train_labels = torch.tensor(annotated_train.train[\"Survived\"].to_numpy())\n",
    "\n",
    "        validation = annotated_train.validation.drop([\"Survived\"], 1).to_numpy()\n",
    "        validation_features = torch.tensor(validation)\n",
    "        validation_outputs = model(validation_features.float())\n",
    "        validation_labels = torch.tensor(annotated_train.validation[\"Survived\"].to_numpy())\n",
    "        loss = criterion(\n",
    "            train_outputs.flatten().float(), \n",
    "            train_labels.flatten().float()\n",
    "            )\n",
    "\n",
    "        validation_loss = criterion(\n",
    "            validation_outputs.flatten().float(), \n",
    "            validation_labels.flatten().float()\n",
    "            )\n",
    "      \n",
    "        yield AnnotatedLoss(loss = loss, validation_loss = validation_loss)\n",
    "    \n",
    "    @artifact\n",
    "    def model (config, annotated_train):\n",
    "        n_features = annotated_train.train.drop([\"Survived\"], 1).shape[1]\n",
    "        model = torch.nn.Sequential(torch.nn.Linear(n_features, 50),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(50, 1),\n",
    "                            torch.nn.Sigmoid())\n",
    "        return model\n",
    "        \n",
    "build_pipeline(Titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dccd5656",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lf/685_7_hx2bzf7mx7c21bqj9m0000gn/T/ipykernel_79551/2125774132.py:120: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  n_features = annotated_train.train.drop([\"Survived\"], 1).shape[1]\n",
      "/var/folders/lf/685_7_hx2bzf7mx7c21bqj9m0000gn/T/ipykernel_79551/2125774132.py:97: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  train = annotated_train.train.drop([\"Survived\"], 1).to_numpy()\n",
      "/var/folders/lf/685_7_hx2bzf7mx7c21bqj9m0000gn/T/ipykernel_79551/2125774132.py:102: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  validation = annotated_train.validation.drop([\"Survived\"], 1).to_numpy()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnotatedLoss(loss=tensor(0.6893, grad_fn=<BinaryCrossEntropyBackward0>), validation_loss=tensor(0.6893, grad_fn=<BinaryCrossEntropyBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(Titanic.train.generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d1d336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (3): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Titanic.model.artifact()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517c0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
